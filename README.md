![Athena Banner](./docs/athena_banner.png)

> **Last Updated**: 03 January 2026

# ğŸ›ï¸ Athena: AI-Powered Personal Knowledge System

![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)
![Protocols](https://img.shields.io/badge/Protocols-269-blue)
![Sessions](https://img.shields.io/badge/Sessions-560-green)

![Python](https://img.shields.io/badge/Python-3.14.2-3776AB?logo=python&logoColor=white)
![Built with Claude](https://img.shields.io/badge/Reasoning-Claude_Opus_4.5-CC785C?logo=anthropic)
![Built with Gemini](https://img.shields.io/badge/Gemini-3.0_Pro-4285F4?logo=google)
![IDE](https://img.shields.io/badge/IDE-Antigravity-000000?logo=google)

> **Development Environment**: [Google Antigravity](https://antigravity.google/) â€” an agentic IDE that allows AI to read/write files directly. Antigravity supports multiple reasoning models (Claude, Gemini, GPT); this system primarily uses **Claude Opus 4.5** as the reasoning engine.

---

## Why This Matters

If you're using AI for anything beyond one-off questions, you've probably hit the same wall: **every session starts from zero**.

Yes, ChatGPT has memory now. So does Claude. But their memory is **platform-locked**. If you switch models, you lose everything. If the platform changes their memory policy, you lose everything.

Athena is different: **portable, platform-agnostic memory**. Your context lives in Markdown files you own. You can take it to any model, any platform, any time. That's the moat.

---

## The Problem

**I got tired of paying for amnesia.**

Every new chat session was a cold start. I was pasting a ~50k-token "identity + context" prompt just to get consistent answers. The best insights from previous sessions? Trapped in old transcripts I'd never find again.

| Pain Point | What It Cost Me |
|------------|-----------------|
| **No memory** | Repeating the same context every session |
| **Lost decisions** | Couldn't remember *why* I'd decided X in Session 19 |
| **Context limits** | 50k tokens of manual paste just to "remind" the AI who I was |
| **Platform lock-in** | Switching models meant losing all accumulated context |

---

## The Process (The Schlep)

Here's what I actually did. No shortcuts.

> **Key insight**: I didn't build this alone. The entire system was **co-developed with AI** â€” Claude and Gemini working alongside me in real-time. Every protocol, every architecture decision, every refactor was a collaborative iteration. That's what makes this approach powerful: the AI helps build the system that makes the AI more useful.

### Phase 1: Tool Selection (Week 1)

- Evaluated agentic IDEs (Cursor, Continue, Aider, Antigravity) â€” chose Antigravity for native Gemini integration and long context window
- Set up a Supabase project with pgvector for vector embeddings
- Configured `.env` with API keys (see [Prerequisites](#prerequisites))

### Phase 2: Architecture (Weeks 2-4)

- Designed the directory structure *with AI* (`.framework/` for laws, `.context/` for memories, `.agent/` for scripts)
- Built the core loop together: `/start` (boot) â†’ Work â†’ `/end` (commit)
- Created the first 10 protocols â€” reusable decision frameworks extracted from our collaborative thinking

### Phase 3: Data Feeding (Ongoing)

- Fed it personal knowledge: decision logs, case studies, business frameworks, session transcripts
- Tagged and indexed files for retrieval (`TAG_INDEX.md`)
- Built `supabase_sync.py` to push Markdown to vector embeddings (or keep local for sensitive data)

### Phase 4: Continuous Iteration (560+ Sessions)

| Session Range | What Changed |
|---------------|--------------|
| 1-50 | Basic boot/end cycle, first protocols |
| 50-150 | Semantic search added, hybrid RAG |
| 150-300 | Cross-encoder reranking, RRF fusion |
| 300-400 | SDK refactor (`athena` package), typing, tests |
| 400-500+ | Trilateral feedback, governance audit, external red-teaming |

**The pattern**: Every friction became a protocol. Every failure became a case study. The AI helped document its own evolution.

### What the Schlep Looked Like

```
â”œâ”€â”€ 560 sessions logged (human + AI collaboration)
â”œâ”€â”€ 269 protocols extracted
â”œâ”€â”€ 117 automation scripts written
â”œâ”€â”€ 3 major refactors (monolith â†’ SDK)
â”œâ”€â”€ 2 external red-team audits
â””â”€â”€ Countless errors, dead ends, and "why isn't this working" nights
```

---

## The Result

### Quantitative (What Changed)

| Metric | Before | After |
|--------|--------|-------|
| **Context injection** | ~50k tokens (manual copy-paste per session) | **~2k tokens** (auto-retrieved summary) |
| **Boot time** | 2-3 minutes | **<30 seconds** |
| **Session logging** | Insights are manually logged at the end of each session | **Auto-logged** when I hit `/end` |

**What this means in practice:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         BEFORE â†’ AFTER COMPARISON                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚           âŒ BEFORE               â”‚              âœ… AFTER                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ“‹ Paste 50k tokens manually    â”‚  ğŸ” ~2k tokens auto-retrieved             â”‚
â”‚     every session                â”‚     (semantic search)                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â±ï¸  2-3 min boot                â”‚  âš¡ /start boots in <30 seconds           â”‚
â”‚     (manual setup)               â”‚     (automated)                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ“ Process â†’ Store â†’ Integrate  â”‚  ğŸ”„ /end auto-logs, processes & stores   â”‚
â”‚     (weekly tedium)              â”‚     (one command)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Qualitative (What It Means)

| Pillar | Outcome |
|--------|---------|
| **Recursive Self-Improvement (RSI)** | Driven by both human and AI â€” we feed off each other's insights. I stopped *recreating* context and started *compounding* it. Every session builds on the last. |
| **Portability** | Data lives locally (primary) and in the cloud. Not trapped in ChatGPT or Claude. It's mine â€” I can port it anywhere. |
| **Principles** | 269 protocols + case studies extracted from my own decisions â€” stored principles I can reuse and refine. Like Ray Dalio's systematized learnings, but for AI collaboration. |

### Proof It Works

In Session 400, Athena recalled a trading risk limit I'd set in Session 19 â€” months earlier â€” and flagged it before I repeated an old mistake.

```
â”œâ”€â”€ Query: "position sizing rules"
â”œâ”€â”€ Retrieved: protocols/trading/risk_limits.md (similarity: 0.89)
â”œâ”€â”€ Created: 2025-03-14 | Last accessed: 2025-12-28
â””â”€â”€ Injected: "Max daily loss: 2% of account. Hard stop."
```

A generic chat assistant would have missed it. Athena didn't.

---

## What I Learned

| Insight | Principle |
|---------|----------|
| **Co-development is the unlock** | Building *with* AI, not just *using* AI, creates compounding returns. |
| **Portable memory beats platform memory** | Own your context. Don't rent it from OpenAI or Anthropic. |
| **Retrieval is end-to-end** | Simple RAG fails on broad queries. RRF fusion + reranking solved quality/latency tradeoff. |
| **Protocols beat prompts** | Reusable decision frameworks outlast one-shot prompt engineering. |
| **Ship at 70%** | Perfectionism kills velocity. Iterate in production. |

---

## Why This Matters (Beyond Me)

This isn't about building *my* assistant. It's about proving a pattern:

1. **Portable memory is the real unlock** â€” ChatGPT and Claude have memory now, but it's locked to their platforms. Athena's memory is *yours* â€” Markdown files you can take to any model.
2. **Co-development is the future** â€” The 500+ sessions of iteration weren't me instructing an AI. They were me *building with* an AI. That's a different paradigm.
3. **Your context is your moat** â€” The knowledge you feed the system is unique to you. That's unforkable.

---

## ğŸ›¡ï¸ The Most Powerful Feature: Trilateral Feedback Loop

> **One AI is not enough for life decisions.**

This is Athena's biggest unlock: **cross-model validation that reduces hallucination risk to near-zero (<1%)**.

Any single AI has blind spots. The most dangerous outcome is accepting AI output on *important decisions* without external validation. When 3-4 independent LLMs with different training data all converge on the same conclusion, you've found robust signal. When they disagree, you've found exactly where to dig deeper.

**The solution: 3+ independent AIs with different training data.**

```mermaid
flowchart LR
    A[You] -->|1. Query| B["Athena<br/>(Claude)"]
    B -->|2. Discuss| A
    A -->|3. Export Artifact| C["AI #2<br/>Gemini"]
    A -->|3. Export Artifact| D["AI #3<br/>ChatGPT"]
    A -->|3. Export Artifact| E["AI #4<br/>Grok"]
    C -->|4. Red-Team Audit| F[Findings]
    D -->|4. Red-Team Audit| F
    E -->|4. Red-Team Audit| F
    F -->|5. Return| B
    B -->|6. Synthesize| G[Final Conclusion]
    
    style A fill:#4a9eff,color:#fff
    style B fill:#cc785c,color:#fff
    style C fill:#4285f4,color:#fff
    style D fill:#10a37f,color:#fff
    style E fill:#1da1f2,color:#fff
    style G fill:#22c55e,color:#fff
```

**Why different models matter**: Each AI has different training data, different biases, different blind spots. When 3 independent models agree, you've found a robust pattern. When they disagree, you've found where to dig deeper.

**The Loop**: Query your primary AI â†’ Discuss â†’ Export artifact to 3 independent LLMs (different providers) â†’ Collect red-team findings â†’ **You + Athena synthesize together** â†’ Final conclusion.

| Risk Level | Examples | Validation |
|------------|----------|------------|
| Low | Code refactoring | Optional |
| High | Financial, relationship | **Mandatory (3 AIs)** |
| Critical | Legal, health | **Mandatory + Human Expert** |

> **Rule of Thumb**: If you'd regret it for more than a week if wrong â†’ run trilateral feedback.

ğŸ‘‰ **[Full protocol + red-team prompt â†’](docs/TRILATERAL_FEEDBACK.md)**

---

## ğŸ“š Further Reading

<details>
<summary><strong>ğŸ”’ Security Model</strong></summary>

### Data Residency Options

| Mode | Where Data Lives | Best For |
|------|------------------|----------|
| **Cloud** | Supabase (your project) | Cross-device access, collaboration |
| **Local** | Your machine only | Sensitive data, air-gapped environments |
| **Hybrid** | Local files + cloud embeddings | Best of both (embeddings only leave machine) |

> **Sensitive data?** Keep it local. The `athena` SDK supports local vector stores (ChromaDB, LanceDB) for users who don't want data leaving their machine. See [docs/LOCAL_MODE.md](docs/LOCAL_MODE.md).

### What Leaves Your Machine (Cloud Mode)

| Component | Sends Raw Text? | Sends Embeddings? | Destination |
|-----------|-----------------|-------------------|-------------|
| **Embedding API** | Yes (text chunks) | â€” | Google Cloud |
| **LLM API** | Yes (prompts) | â€” | Anthropic (Claude) |
| **Supabase** | No | Yes (vectors only) | Your Supabase project |

### Key Security Practices

- **Supabase Keys**: Use `SUPABASE_ANON_KEY` for client-side operations. Never expose `SUPABASE_SERVICE_ROLE_KEY` in code or logs.
- **Row-Level Security**: Enable RLS on Supabase tables. See [docs/SECURITY.md](docs/SECURITY.md) for policy templates.
- **Agentic Safety**: If using an agentic IDE with filesystem access, restrict the agent's working directory. Never grant access to `~/.ssh`, `.env` files, or git credentials.

</details>

<details>
<summary><strong>âš™ï¸ Prerequisites & Quick Start</strong></summary>

### Prerequisites

- Python 3.10+
- Supabase project with pgvector enabled ([setup guide](docs/GETTING_STARTED.md)) â€” *or use local mode*
- API keys in `.env`:

```bash
# Required
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_ANON_KEY=your-anon-key  # NOT service_role key
ANTHROPIC_API_KEY=your-anthropic-key  # For Claude reasoning

# Optional (for trilateral feedback with multiple LLMs)
GOOGLE_API_KEY=your-google-api-key
OPENAI_API_KEY=your-openai-key
```

```bash
cp .env.example .env
# Add your keys to .env
```

### Quick Start

```bash
# Clone
git clone https://github.com/winstonkoh87/Athena-Public.git
cd Athena-Public

# Install (minimal)
pip install -e .

# Or with full dependencies (vector search + reranking)
pip install -e ".[full]"

# Test installation
python examples/quickstart/01_boot.py

# Try the search demo
python examples/quickstart/02_search.py "position sizing"
```

See [examples/quickstart/](examples/quickstart/) for runnable demos.

</details>

<details>
<summary><strong>ğŸ› ï¸ Tech Stack & Architecture</strong></summary>

### Tech Stack

| Layer | Technology | Purpose |
|-------|------------|---------|
| **SDK** | `athena` Python package | Core search, reranking, memory |
| **Reasoning** | Claude Opus 4.5 (primary) | Main reasoning engine |
| **IDE** | Antigravity (supports Claude, Gemini, GPT) | Agentic development environment |
| **Embeddings** | `text-embedding-004` (768-dim) | Google embedding model |
| **Memory** | Supabase + pgvector *or* local (ChromaDB) | Vector database |
| **Knowledge Store** | Markdown files (git-versioned) | Human-readable, locally owned |

### The Core Loop

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                         â”‚
â”‚   (1) /start â”€â”€â–º Retrieve Context â”€â”€â–º (2) Work â”€â”€â–º (3) /end             â”‚
â”‚       â–²                                                    â”‚            â”‚
â”‚       â”‚                                                    â–¼            â”‚
â”‚       â””â”€â”€â”€â”€â”€ (5) Next Session â—„â”€â”€ Embed â—„â”€â”€ (4) Extract & Store        â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Think of it like **Git, but for conversations**. Each session builds on the last. Important decisions get captured, indexed, and recoverable.

### What Athena Does

| Feature | How It Works |
|---------|--------------|
| **`/start` boot** | Loads identity + retrieves relevant context from long-term memory |
| **`/end` commit** | Summarizes session, extracts decisions, saves to knowledge store |
| **Hybrid search** | Fuses Canonical Memory + Tags + Vectors + Filenames via RRF |
| **Cross-encoder reranking** | Refines top results with `sentence-transformers` |
| **Protocol library** | 269 reusable playbooks (trading, writing, design) |

### Repository Structure

```
Athena-Public/
â”œâ”€â”€ src/athena/           # SDK package (pip installable)
â”‚   â”œâ”€â”€ core/             #    Config, models
â”‚   â”œâ”€â”€ tools/            #    Search, reranker, latency
â”‚   â””â”€â”€ memory/           #    Vector DB interface
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ quickstart/       # Runnable demos
â”‚   â”œâ”€â”€ scripts/          # Automation scripts
â”‚   â”œâ”€â”€ protocols/        # Thinking patterns (starter pack included)
â”‚   â”œâ”€â”€ workflows/        # Slash commands
â”‚   â””â”€â”€ templates/        # Starter templates
â”œâ”€â”€ docs/                 # Deep documentation
â”œâ”€â”€ community/            # Contributing, roadmap
â”œâ”€â”€ pyproject.toml        # Modern packaging
â””â”€â”€ .env.example          # Environment template
```

</details>

<details>
<summary><strong>ğŸ“– Key Concepts & Workflows</strong></summary>

### Key Concepts

- **[The Architecture](docs/ARCHITECTURE.md)** â€” How the system is designed
- **[VectorRAG](docs/VECTORRAG.md)** â€” Semantic memory implementation
- **[Getting Started](docs/GETTING_STARTED.md)** â€” Build your own
- **[Reasoning Modes](examples/concepts/adaptive_latency.md)** â€” `/start`, `/think`, `/ultrathink`
- **[Case Study: BCM](examples/case_studies/CS-140-bcm-silent-partner-analysis.md)** â€” Real-world due diligence example
- **[Glossary](docs/GLOSSARY.md)** â€” Key terms and definitions

### Example Workflows

| Command | Description |
|---------|-------------|
| `/start` | Boot system, load identity |
| `/end` | Close session, commit to memory |
| `/think` | Deep reasoning mode |
| `/ultrathink` | Maximum depth analysis |
| `/refactor` | Workspace optimization |
| `/research` | Multi-source web research |

See [examples/workflows/](examples/workflows/) for full list.

</details>

---

## About Me

I'm Winston â€” a systems thinker who spent 10+ years in financial services before pivoting to AI engineering.

This project represents my journey from "user" to "builder" â€” a production-grade personal infrastructure I rely on daily, co-developed with AI from day one.

- **GitHub**: [@winstonkoh87](https://github.com/winstonkoh87)
- **Portfolio**: [winstonkoh87.github.io](https://winstonkoh87.github.io)

---

## License

MIT License â€” see [LICENSE](LICENSE)

---

<details>
<summary><strong>ğŸ“‹ Changelog</strong></summary>

- **v1.2.4** (Jan 2026): README restructure â€” collapsed technical sections into "Further Reading"
- **v1.2.3** (Jan 2026): Stats correction â€” 269 protocols, 538 sessions, 117 scripts
- **v1.2.2** (Jan 2026): Stats sync â€” 248 protocols, 560 sessions, 97 scripts; removed off-topic content
- **v1.2.1** (Jan 2026): README overhaul â€” Process section, Security Model, co-development narrative
- **v1.2.0** (Jan 2026): New year sync â€” 246 protocols, 511 sessions
- **v1.1.0** (Dec 2025): Year-end sync â€” 238 protocols, 489 sessions
- **v1.0.0**: SDK architecture (`src/athena/`), quickstart examples

</details>

---

*For the full documentation, case studies, and deep dives, see [docs/](docs/).*
